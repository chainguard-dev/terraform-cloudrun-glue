receivers:
  prometheus:
    config:
      scrape_configs:
      - job_name: "localhost"
        scrape_interval: 10s
        static_configs:
        # TODO: make this configurable
        - targets: ["localhost:2112"]
        metric_relabel_configs:
          - source_labels: [ __name__ ]
            regex: '^prometheus_.*'
            action: drop
          - source_labels: [ __name__ ]
            regex: '^process_.*'
            action: drop
          - source_labels: [ __name__ ]
            regex: '^go_.*'
            action: drop

processors:
  batch:
    # batch metrics before sending to reduce API usage
    send_batch_max_size: 200
    send_batch_size: 200
    timeout: 5s

  memory_limiter:
    # drop metrics if memory usage gets too high
    check_interval: 1s
    limit_percentage: 65
    spike_limit_percentage: 20

  # automatically detect Cloud Run resource metadata
  resourcedetection:
    detectors: [env, gcp]
    timeout: 2s
    override: false

  resource:
    attributes:
      # add instance_id as a resource attribute
    - key: service.instance.id
      from_attribute: faas.id
      action: upsert
      # parse service name from K_SERVICE Cloud Run variable
    - key: service.name
      value: ${env:K_SERVICE}
      action: insert

exporters:
  googlemanagedprometheus:
    sending_queue:
      # Having multiple consumers cause out-of-order time series,
      # which GMP doesn't like. Since we are scraping a single host
      # we shouldn't need too many consumers anyway.
      num_consumers: 1

extensions:
  health_check:

service:
  telemetry:
    logs:
      # We don't want to see scraper startup logging every
      # cold start.
      level: "error"
  extensions: [health_check]
  pipelines:
    metrics:
      receivers: [prometheus]
      processors: [batch, memory_limiter, resourcedetection, resource]
      exporters: [googlemanagedprometheus]
